{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc256b2-765e-4244-8d01-e7aaabd7275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IuG_Lap1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\IuG_Lap1\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4778cfb2-4f3f-4af2-b15a-60a32ae040d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_IDS_ST: Dict[str, str] = {\n",
    "    \"JobBERT-v2\": \"TechWolf/JobBERT-v2\",\n",
    "    \"JobBERT-v3\": \"TechWolf/JobBERT-v3\",\n",
    "    \"MiniLM-job-matcher\": \"forestav/job_matching_sentence_transformer\",\n",
    "}\n",
    "\n",
    "# LoRA model configuration\n",
    "LORA_MODEL_NAME = \"Fair-Resume-LoRA\"\n",
    "LORA_BASE = \"BAAI/bge-large-en-v1.5\"\n",
    "LORA_ADAPTER = \"renhehuang/fair-resume-job-matcher-lora\"\n",
    "\n",
    "OUTPUT_DIR = \"seniority_gender_similarity_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baba34e6-641f-4e06-a221-3ca4fbdc2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENIORITY_TERMS = [\"junior\", \"mid-level\", \"senior\", \"lead\"]\n",
    "\n",
    "# Example subset of names; replace with names from your actual dataset if you prefer\n",
    "FEMALE_NAMES = [\"Fatma Ibrahim\", \"Priya Sharma\", \"Nyambura Wambui\", \"Emily Johnson\", \"Daniela PÃ©rez\", \"Kenji Yamamoto\"]\n",
    "MALE_NAMES = [\"Ahmed Hassan\", \"Rajesh Patel\", \"Tunde Adebayo\", \"Robert Miller\", \"Miguel Santos\", \"Mei Zhang\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6299cd4-e1d8-4fe0-aca9-593134a5c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_st_model(model_id: str) -> SentenceTransformer:\n",
    "    print(f\"\\nLoading SentenceTransformer model: {model_id}\")\n",
    "    model = SentenceTransformer(model_id, device=\"cpu\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def encode_with_st(model: SentenceTransformer, texts: List[str]) -> np.ndarray:\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "def load_lora_model() -> Dict:\n",
    "    print(f\"\\nLoading LoRA model: base={LORA_BASE}, adapter={LORA_ADAPTER}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LORA_BASE)\n",
    "    base_model = AutoModel.from_pretrained(LORA_BASE)\n",
    "    model_bge = PeftModel.from_pretrained(base_model, LORA_ADAPTER)\n",
    "    model_bge.eval()\n",
    "    model_bge.to(\"cpu\")\n",
    "    return {\"tokenizer\": tokenizer, \"model\": model_bge}\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # last hidden state: (batch, seq_len, dim)\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1)\n",
    "        .expand(token_embeddings.size())\n",
    "        .float()\n",
    "    )\n",
    "    summed = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "    counts = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "\n",
    "def encode_with_lora(lora_bundle: Dict, texts: List[str]) -> np.ndarray:\n",
    "    tokenizer = lora_bundle[\"tokenizer\"]\n",
    "    model = lora_bundle[\"model\"]\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "        pooled = mean_pooling(output, encoded[\"attention_mask\"])\n",
    "        # L2 normalize\n",
    "        pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "    return pooled.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77113ac8-2f0b-406a-a609-90e6acbaafa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    # embeddings are L2-normalized, so cosine similarity = dot product\n",
    "    return np.matmul(a, b.T)\n",
    "\n",
    "\n",
    "def compute_gender_seniority_similarities(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    encode_fn,\n",
    ") -> pd.DataFrame:\n",
    "    all_seniority = SENIORITY_TERMS\n",
    "    all_female = FEMALE_NAMES\n",
    "    all_male = MALE_NAMES\n",
    "\n",
    "    # Encode sets\n",
    "    seniority_embs = encode_fn(all_seniority)\n",
    "    female_embs = encode_fn(all_female)\n",
    "    male_embs = encode_fn(all_male)\n",
    "\n",
    "    # Similarity matrices: shape (num_seniority, num_names)\n",
    "    sim_female = cosine_sim_matrix(seniority_embs, female_embs)\n",
    "    sim_male = cosine_sim_matrix(seniority_embs, male_embs)\n",
    "\n",
    "    female_mean = sim_female.mean(axis=1)\n",
    "    male_mean = sim_male.mean(axis=1)\n",
    "\n",
    "    rows = []\n",
    "    for i, term in enumerate(all_seniority):\n",
    "        rows.append(\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"model_id\": model_id,\n",
    "                \"seniority_term\": term,\n",
    "                \"avg_sim_female\": float(female_mean[i]),\n",
    "                \"avg_sim_male\": float(male_mean[i]),\n",
    "                \"diff_male_minus_female\": float(male_mean[i] - female_mean[i]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395fffbb-8712-41c2-80eb-a311a99e5b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading SentenceTransformer model: TechWolf/JobBERT-v2\n",
      "\n",
      "Saved per-model results to: seniority_gender_similarity_results\\seniority_gender_similarity_JobBERT-v2.csv\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | model_name   | model_id            | seniority_term   |   avg_sim_female |   avg_sim_male |   diff_male_minus_female |\n",
       "|---:|:-------------|:--------------------|:-----------------|-----------------:|---------------:|-------------------------:|\n",
       "|  0 | JobBERT-v2   | TechWolf/JobBERT-v2 | junior           |         0.250747 |       0.286732 |                0.0359853 |\n",
       "|  1 | JobBERT-v2   | TechWolf/JobBERT-v2 | mid-level        |         0.252749 |       0.297305 |                0.0445564 |\n",
       "|  2 | JobBERT-v2   | TechWolf/JobBERT-v2 | senior           |         0.298549 |       0.376541 |                0.0779927 |\n",
       "|  3 | JobBERT-v2   | TechWolf/JobBERT-v2 | lead             |         0.235837 |       0.274788 |                0.0389515 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading SentenceTransformer model: TechWolf/JobBERT-v3\n",
      "\n",
      "Saved per-model results to: seniority_gender_similarity_results\\seniority_gender_similarity_JobBERT-v3.csv\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | model_name   | model_id            | seniority_term   |   avg_sim_female |   avg_sim_male |   diff_male_minus_female |\n",
       "|---:|:-------------|:--------------------|:-----------------|-----------------:|---------------:|-------------------------:|\n",
       "|  0 | JobBERT-v3   | TechWolf/JobBERT-v3 | junior           |         0.236771 |       0.25839  |                0.0216185 |\n",
       "|  1 | JobBERT-v3   | TechWolf/JobBERT-v3 | mid-level        |         0.194596 |       0.222111 |                0.0275154 |\n",
       "|  2 | JobBERT-v3   | TechWolf/JobBERT-v3 | senior           |         0.15339  |       0.208103 |                0.0547123 |\n",
       "|  3 | JobBERT-v3   | TechWolf/JobBERT-v3 | lead             |         0.148594 |       0.175524 |                0.0269301 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading SentenceTransformer model: forestav/job_matching_sentence_transformer\n",
      "\n",
      "Saved per-model results to: seniority_gender_similarity_results\\seniority_gender_similarity_MiniLM-job-matcher.csv\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | model_name         | model_id                                   | seniority_term   |   avg_sim_female |   avg_sim_male |   diff_male_minus_female |\n",
       "|---:|:-------------------|:-------------------------------------------|:-----------------|-----------------:|---------------:|-------------------------:|\n",
       "|  0 | MiniLM-job-matcher | forestav/job_matching_sentence_transformer | junior           |         0.193301 |       0.199456 |               0.00615457 |\n",
       "|  1 | MiniLM-job-matcher | forestav/job_matching_sentence_transformer | mid-level        |         0.166189 |       0.149165 |              -0.0170247  |\n",
       "|  2 | MiniLM-job-matcher | forestav/job_matching_sentence_transformer | senior           |         0.185466 |       0.206135 |               0.0206698  |\n",
       "|  3 | MiniLM-job-matcher | forestav/job_matching_sentence_transformer | lead             |         0.156653 |       0.182317 |               0.0256643  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading LoRA model: base=BAAI/bge-large-en-v1.5, adapter=renhehuang/fair-resume-job-matcher-lora\n",
      "\n",
      "Saved LoRA results to: seniority_gender_similarity_results\\seniority_gender_similarity_Fair-Resume-LoRA.csv\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | model_name       | model_id                                                       | seniority_term   |   avg_sim_female |   avg_sim_male |   diff_male_minus_female |\n",
       "|---:|:-----------------|:---------------------------------------------------------------|:-----------------|-----------------:|---------------:|-------------------------:|\n",
       "|  0 | Fair-Resume-LoRA | BAAI/bge-large-en-v1.5+renhehuang/fair-resume-job-matcher-lora | junior           |         0.492073 |       0.501049 |               0.00897595 |\n",
       "|  1 | Fair-Resume-LoRA | BAAI/bge-large-en-v1.5+renhehuang/fair-resume-job-matcher-lora | mid-level        |         0.5215   |       0.51267  |              -0.00883037 |\n",
       "|  2 | Fair-Resume-LoRA | BAAI/bge-large-en-v1.5+renhehuang/fair-resume-job-matcher-lora | senior           |         0.523085 |       0.532521 |               0.00943542 |\n",
       "|  3 | Fair-Resume-LoRA | BAAI/bge-large-en-v1.5+renhehuang/fair-resume-job-matcher-lora | lead             |         0.478318 |       0.487053 |               0.00873452 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined results saved to: seniority_gender_similarity_results\\seniority_gender_similarity_all_models.csv\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|    | model_name         | model_id                                                       | seniority_term   |   avg_sim_female |   avg_sim_male |   diff_male_minus_female |\n",
       "|---:|:-------------------|:---------------------------------------------------------------|:-----------------|-----------------:|---------------:|-------------------------:|\n",
       "|  0 | JobBERT-v2         | TechWolf/JobBERT-v2                                            | junior           |         0.250747 |       0.286732 |               0.0359853  |\n",
       "|  1 | JobBERT-v2         | TechWolf/JobBERT-v2                                            | mid-level        |         0.252749 |       0.297305 |               0.0445564  |\n",
       "|  2 | JobBERT-v2         | TechWolf/JobBERT-v2                                            | senior           |         0.298549 |       0.376541 |               0.0779927  |\n",
       "|  3 | JobBERT-v2         | TechWolf/JobBERT-v2                                            | lead             |         0.235837 |       0.274788 |               0.0389515  |\n",
       "|  4 | JobBERT-v3         | TechWolf/JobBERT-v3                                            | junior           |         0.236771 |       0.25839  |               0.0216185  |\n",
       "|  5 | JobBERT-v3         | TechWolf/JobBERT-v3                                            | mid-level        |         0.194596 |       0.222111 |               0.0275154  |\n",
       "|  6 | JobBERT-v3         | TechWolf/JobBERT-v3                                            | senior           |         0.15339  |       0.208103 |               0.0547123  |\n",
       "|  7 | JobBERT-v3         | TechWolf/JobBERT-v3                                            | lead             |         0.148594 |       0.175524 |               0.0269301  |\n",
       "|  8 | MiniLM-job-matcher | forestav/job_matching_sentence_transformer                     | junior           |         0.193301 |       0.199456 |               0.00615457 |\n",
       "|  9 | MiniLM-job-matcher | forestav/job_matching_sentence_transformer                     | mid-level        |         0.166189 |       0.149165 |              -0.0170247  |\n",
       "| 10 | MiniLM-job-matcher | forestav/job_matching_sentence_transformer                     | senior           |         0.185466 |       0.206135 |               0.0206698  |\n",
       "| 11 | MiniLM-job-matcher | forestav/job_matching_sentence_transformer                     | lead             |         0.156653 |       0.182317 |               0.0256643  |\n",
       "| 12 | Fair-Resume-LoRA   | BAAI/bge-large-en-v1.5+renhehuang/fair-resume-job-matcher-lora | junior           |         0.492073 |       0.501049 |               0.00897595 |\n",
       "| 13 | Fair-Resume-LoRA   | BAAI/bge-large-en-v1.5+renhehuang/fair-resume-job-matcher-lora | mid-level        |         0.5215   |       0.51267  |              -0.00883037 |\n",
       "| 14 | Fair-Resume-LoRA   | BAAI/bge-large-en-v1.5+renhehuang/fair-resume-job-matcher-lora | senior           |         0.523085 |       0.532521 |               0.00943542 |\n",
       "| 15 | Fair-Resume-LoRA   | BAAI/bge-large-en-v1.5+renhehuang/fair-resume-job-matcher-lora | lead             |         0.478318 |       0.487053 |               0.00873452 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "all_results = []\n",
    "\n",
    "# SentenceTransformer models\n",
    "for model_name, model_id in MODEL_IDS_ST.items():\n",
    "    st_model = load_st_model(model_id)\n",
    "\n",
    "    def encode_fn(texts: List[str], _m=st_model):\n",
    "        return encode_with_st(_m, texts)\n",
    "\n",
    "    df_model = compute_gender_seniority_similarities(\n",
    "        model_name=model_name,\n",
    "        model_id=model_id,\n",
    "        encode_fn=encode_fn,\n",
    "    )\n",
    "    all_results.append(df_model)\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        f\"seniority_gender_similarity_{model_name.replace(' ', '_')}.csv\",\n",
    "    )\n",
    "    df_model.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSaved per-model results to: {out_path}\")\n",
    "    display(Markdown(df_model.to_markdown()))\n",
    "\n",
    "# LoRA model (bge-large-en + fairness adapter)\n",
    "lora_bundle = load_lora_model()\n",
    "\n",
    "def encode_fn_lora(texts: List[str], bundle=lora_bundle):\n",
    "    return encode_with_lora(bundle, texts)\n",
    "\n",
    "df_lora = compute_gender_seniority_similarities(\n",
    "    model_name=LORA_MODEL_NAME,\n",
    "    model_id=f\"{LORA_BASE}+{LORA_ADAPTER}\",\n",
    "    encode_fn=encode_fn_lora,\n",
    ")\n",
    "all_results.append(df_lora)\n",
    "\n",
    "lora_out = os.path.join(\n",
    "    OUTPUT_DIR,\n",
    "    f\"seniority_gender_similarity_{LORA_MODEL_NAME.replace(' ', '_')}.csv\",\n",
    ")\n",
    "df_lora.to_csv(lora_out, index=False)\n",
    "print(f\"\\nSaved LoRA results to: {lora_out}\")\n",
    "display(Markdown(df_lora.to_markdown()))\n",
    "\n",
    "# Combined CSV\n",
    "df_all = pd.concat(all_results, ignore_index=True)\n",
    "combined_path = os.path.join(\n",
    "    OUTPUT_DIR, \"seniority_gender_similarity_all_models.csv\"\n",
    ")\n",
    "df_all.to_csv(combined_path, index=False)\n",
    "\n",
    "print(f\"\\nCombined results saved to: {combined_path}\")\n",
    "display(Markdown(df_all.to_markdown()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da240403-75c2-429b-b613-dade739e53fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
